# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mmm8P9wOxA4AZ86_U3I90pGF-y5I9GO
"""

import re
import json
import ast
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from sklearn.metrics.pairwise import cosine_similarity
from yellowbrick.text import TSNEVisualizer
import warnings
warnings.filterwarnings('ignore')
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
#from gensim.utils import simple_preprocess
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import label_binarize

restaurants = pd.read_csv('Res_dataset.csv', encoding='latin1')

# Cleaned Restaurant name by removing unwanted 


def clean_restaurant_name(name):
    if isinstance(name, str):
        # Removing non-alphabetic characters and extra spaces
        cleaned_name = re.sub(r'[^a-zA-Z\s]', '', name)
        cleaned_name = re.sub(r'\s+', ' ', cleaned_name)
        # Converting to lowercase
        cleaned_name = cleaned_name.lower()
        return cleaned_name.strip()  # Removing leading/trailing spaces
    else:
        return ''

restaurants['Cleaned_Restaurant_Name'] = restaurants['Restaurant_Name'].apply(clean_restaurant_name)

# Printing cleaned restaurant names
for name in restaurants['Cleaned_Restaurant_Name']:
    print(name)

# Cleaned restaurant descrtption to remove commas and unwanted characters

def clean_description(text):
    if isinstance(text, str):
      # Removal of characters and extra spaces using regular expressions
        cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        cleaned_text = cleaned_text.lower()
        return cleaned_text.strip()
    else:
        return ''

#Cleaning specifice columns
restaurants['Cleaned_Restaurant_description'] = restaurants['Restaurant_description'].apply(clean_description)

# Iterating over the cleaned locations and print
for Restaurant_description in restaurants['Cleaned_Restaurant_description']:
    print(Restaurant_description)

# Cleaning location to remove comma between city and country

def clean_location(location):
    # Striping leading and trailing spaces
    cleaned_location = location.strip()
    # Converting to lowercase
    cleaned_location = cleaned_location.lower()
    # Replacing commas with empty string
    cleaned_location = cleaned_location.replace(',', '')
    return cleaned_location

# assigning the 'Location' column to new column "clean_Location"
restaurants['Cleaned_Location'] = restaurants['Location'].apply(clean_location)

# Iterating over the cleaned locations and print
for location in restaurants['Cleaned_Location']:
    print(location)

###############################correction
def clean_restaurant_type(type_str):

    cleaned_types = [t.strip() for t in type_str.split(',')]
    cleaned_type_str = ', '.join(cleaned_types)
    return cleaned_type_str


restaurants['Cleaned_Restaurant_Type'] = restaurants['Restaurant_type'].apply(clean_restaurant_type)


print(restaurants[['Restaurant_type', 'Cleaned_Restaurant_Type']].head())

#############################correction

def clean_recommended_dishes(dishes_str):

    cleaned_dishes = [dish.strip() for dish in dishes_str.split(',')]
    cleaned_dishes_str = ', '.join(cleaned_dishes)
    return cleaned_dishes_str


restaurants['Cleaned_Recommended_Dishes'] = restaurants['recommended_dishes'].apply(clean_recommended_dishes)


print(restaurants[['recommended_dishes', 'Cleaned_Recommended_Dishes']].head())

def clean_famous_cuisines(cuisines_str):
    cleaned_cuisines = [cuisine.strip() for cuisine in cuisines_str.split(',')]
    cleaned_cuisines_str = ', '.join(cleaned_cuisines)
    return cleaned_cuisines_str


restaurants['Cleaned_Famous_Cuisines'] = restaurants['famous_Cuisines'].apply(clean_famous_cuisines)


print(restaurants[['famous_Cuisines', 'Cleaned_Famous_Cuisines']].head())

# Function to clean the allergy_food_product column
def clean_allergy_food_product(allergy_str):
    # Split the string by commas and strip leading/trailing spaces from each substring
    cleaned_str = [item.strip() for item in allergy_str.split(',')]
    return cleaned_str

# Apply the function to the allergy_food_product column
restaurants['Cleaned_Allergy_Food_Product'] = restaurants['allergy_food_product'].apply(clean_allergy_food_product)

print(restaurants[['allergy_food_product', 'Cleaned_Allergy_Food_Product']].head())

def clean_menu_without_allergy(text):
    items = text.split(',')
    cleaned_items = [item.strip() for item in items]
    cleaned_text = ', '.join(cleaned_items)
    return cleaned_text

restaurants['Cleaned_Menu_Without_Allergy'] = restaurants['menu_without_allergy'].apply(clean_menu_without_allergy)


print(restaurants[['menu_without_allergy', 'Cleaned_Menu_Without_Allergy']].head())

def clean_menu_with_allergy(text):
    # Split the text based on commas
    items = text.split(',')
    # Remove leading and trailing whitespace from each item
    cleaned_items = [item.strip() for item in items]
    # Join the cleaned items back together with commas
    cleaned_text = ', '.join(cleaned_items)
    return cleaned_text


restaurants['Cleaned_Menu_With_Allergy'] = restaurants['menu_with_allergy'].apply(clean_menu_with_allergy)
print(restaurants[['menu_with_allergy', 'Cleaned_Menu_With_Allergy']].head())

def clean_reviews(text):
    if pd.isnull(text):
        return ""

    cleaned_text = ' '.join(text.split())
    cleaned_text = cleaned_text.lower()
    return cleaned_text


restaurants['reviews'] = restaurants['reviews'].fillna('')
restaurants['Cleaned_Reviews'] = restaurants['reviews'].apply(clean_reviews)
print(restaurants[['reviews', 'Cleaned_Reviews']].head())

nltk.download('punkt')

print(restaurants.columns)

#Added all cleaned columns to an array for further use in functions to recommend restaurants

cleaned_column_types = restaurants[['Cleaned_Restaurant_Type',
       'Cleaned_Recommended_Dishes', 'Cleaned_Famous_Cuisines',
       'Cleaned_Allergy_Food_Product', 'Cleaned_Menu_Without_Allergy',
       'Cleaned_Menu_With_Allergy', 'Cleaned_Reviews', 'Cleaned_Location',
       'Cleaned_Restaurant_Name']].dtypes

print(cleaned_column_types)

#Location must be required
#location ,restaurant type and famouse cuisines

def longfamouscusinebasedd(Location, sentence, cuisine_sentence):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()
    restaurants['Cleaned_Restaurant_Type'] = restaurants['Cleaned_Restaurant_Type'].str.lower()
    restaurants['Cleaned_Famous_Cuisines'] = restaurants['Cleaned_Famous_Cuisines'].str.lower()

    sentence = sentence.lower()
    cuisine_sentence = cuisine_sentence.lower()
    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()
    sentence_tokens = word_tokenize(sentence)
    cuisine_tokens = word_tokenize(cuisine_sentence)
    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]
    cuisine_tokens = [lemm.lemmatize(word) for word in cuisine_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants[restaurants['Cleaned_Location'] == Location.lower()].copy()
    similarities = []
    for index, row in reqbased.iterrows():
        # Tokenize the restaurant type and famous cuisines
        temp_tokens = word_tokenize(row['Cleaned_Restaurant_Type'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]
        cuisine_tokens_copy = cuisine_tokens.copy()

        # Check for common tokens in restaurant type
        rvector = set(temp_tokens).intersection(set(sentence_tokens))

        # Check for common tokens in famous cuisines
        for cuisine_token in cuisine_tokens_copy:
            if cuisine_token in row['Cleaned_Famous_Cuisines']:
                rvector.add(cuisine_token)

        similarities.append(len(rvector))

    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    recommendations = []
    for index, row in reqbased.iterrows():
        recommendation = {
            'Restaurant_Name': row['Restaurant_Name'],
            'Restaurant_Description': row['Restaurant_description'],
            'Restaurant_Type': row['Restaurant_type'],
            'Recommended_Dishes': row['recommended_dishes'],
            'Menu_with_Allergy': row['menu_with_allergy'],
            'Menu_without_Allergy': row['menu_without_allergy'],
            'Allergy_Food_Product': row['allergy_food_product'],
            'Cost_per_Person': row['Cost_per_person'],
            'Reviews': row['reviews'],
            'Similarity': row['similarity']
        }
        recommendations.append(recommendation)

    return recommendations





print(restaurants.columns)

def longfamouscusinebasedddd(Location, sentence, cuisine_sentence, Cost_per_person_sentence):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()
    restaurants['Cleaned_Restaurant_Type'] = restaurants['Cleaned_Restaurant_Type'].str.lower()
    restaurants['Cleaned_Famous_Cuisines'] = restaurants['Cleaned_Famous_Cuisines'].str.lower()
    restaurants['Cost_per_person'] = restaurants['Cost_per_person'].str.lower()

    sentence = sentence.lower()
    cuisine_sentence = cuisine_sentence.lower()
    Cost_per_person_sentence = Cost_per_person_sentence.lower()

    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()

    sentence_tokens = word_tokenize(sentence)
    cuisine_tokens = word_tokenize(cuisine_sentence)
    Cost_tokens = word_tokenize(Cost_per_person_sentence)

    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]
    cuisine_tokens = [lemm.lemmatize(word) for word in cuisine_tokens if word.isalnum() and word not in sw]
    Cost_tokens = [lemm.lemmatize(word) for word in Cost_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants[restaurants['Cleaned_Location'] == Location.lower()].copy()
    similarities = []

    for index, row in reqbased.iterrows():
        # Tokenize the restaurant type and famous cuisines
        temp_tokens = word_tokenize(row['Cleaned_Restaurant_Type'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]
        cuisine_tokens_copy = cuisine_tokens.copy()
        Cost_tokens_copy = Cost_tokens.copy()

        # Check for common tokens in restaurant type
        rvector = set(temp_tokens).intersection(set(sentence_tokens))

        # Check for common tokens in famous cuisines
        for cuisine_token in cuisine_tokens_copy:
            if cuisine_token in row['Cleaned_Famous_Cuisines']:
                rvector.add(cuisine_token)

        # Check for common tokens in Cost_per_person
        for Cost_token in Cost_tokens_copy:
            if Cost_token in row['Cost_per_person']:
                rvector.add(Cost_token)

        similarities.append(len(rvector))

    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    recommendations = []
    for index, row in reqbased.iterrows():
        recommendation = {
            'Restaurant_Name': row['Restaurant_Name'],
            'Restaurant_Description': row['Restaurant_description'],
            'Restaurant_Type': row['Restaurant_type'],
            'Recommended_Dishes': row['recommended_dishes'],
            'Menu_with_Allergy': row['menu_with_allergy'],
            'Menu_without_Allergy': row['menu_without_allergy'],
            'Allergy_Food_Product': row['allergy_food_product'],
            'Cost_per_Person': row['Cost_per_person'],
            'Reviews': row['reviews'],
            'Similarity': row['similarity']
        }
        recommendations.append(recommendation)

    return recommendations


    


#part 2
#if user enter long answers
#Location must be required
#location
def locationbased(Location):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()

    # Tokenize location
    Location_tokens = word_tokenize(Location.lower())
    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()
    Location_tokens = [lemm.lemmatize(word) for word in Location_tokens if word.isalnum() and word not in sw]
    cleaned_location = ' '.join(Location_tokens)

    locationbase = restaurants[restaurants['Cleaned_Location'] == cleaned_location]

    if not locationbase.empty:
        hname = locationbase[['Restaurant_Name', 'Restaurant_description', 'Restaurant_type', 'recommended_dishes', 'menu_with_allergy', 'menu_without_allergy', 'allergy_food_product', 'Cost_per_person', 'reviews']]
        recommendations = []
        for index, row in hname.iterrows():
            recommendation = {
                'Restaurant_Name': row['Restaurant_Name'],
                'Restaurant_description': row['Restaurant_description'],
                'Restaurant_type': row['Restaurant_type'],
                'recommended_dishes': row['recommended_dishes'],
                'menu_with_allergy': row['menu_with_allergy'],
                'menu_without_allergy': row['menu_without_allergy'],
                'allergy_food_product': row['allergy_food_product'],
                'Cost_per_person': row['Cost_per_person'],
                'reviews': row['reviews']
            }
            recommendations.append(recommendation)
        return recommendations
    else:
        print('No restaurants Available')
        return None

def longfamouscusinebased(Location, sentence, cuisine_sentence, allergy_food_sentence):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()
    restaurants['Cleaned_Restaurant_Type'] = restaurants['Cleaned_Restaurant_Type'].str.lower()
    restaurants['Cleaned_Famous_Cuisines'] = restaurants['Cleaned_Famous_Cuisines'].str.lower()
    restaurants['allergy_food_product'] = restaurants['allergy_food_product'].str.lower()

    sentence = sentence.lower()
    cuisine_sentence = cuisine_sentence.lower()
    allergy_food_sentence = allergy_food_sentence.lower()

    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()

    sentence_tokens = word_tokenize(sentence)
    cuisine_tokens = word_tokenize(cuisine_sentence)
    allergy_tokens = word_tokenize(allergy_food_sentence)

    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]
    cuisine_tokens = [lemm.lemmatize(word) for word in cuisine_tokens if word.isalnum() and word not in sw]
    allergy_tokens = [lemm.lemmatize(word) for word in allergy_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants[restaurants['Cleaned_Location'] == Location.lower()].copy()
    similarities = []
    for index, row in reqbased.iterrows():
        # Tokenize the restaurant type and famous cuisines
        temp_tokens = word_tokenize(row['Cleaned_Restaurant_Type'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]
        cuisine_tokens_copy = cuisine_tokens.copy()
        allergy_tokens_copy = allergy_tokens.copy()

        # Check for common tokens in restaurant type
        rvector = set(temp_tokens).intersection(set(sentence_tokens))

        # Check for common tokens in famous cuisines
        for cuisine_token in cuisine_tokens_copy:
            if cuisine_token in row['Cleaned_Famous_Cuisines']:
                rvector.add(cuisine_token)

         # Check for common tokens in allergy food products
        for allergy_token in allergy_tokens_copy:
            if allergy_token in row['allergy_food_product']:
                rvector.add(allergy_token)

        similarities.append(len(rvector))


    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    return reqbased[['reviews', 'Restaurant_Name', 'Restaurant_description','Restaurant_type','recommended_dishes','menu_with_allergy','menu_without_allergy','allergy_food_product',
                     'Cost_per_person','reviews','similarity']].head(10)


# Define parameters
location = 'New York City East Village Manhattan'
sentence = 'i want to eat from a Casual Dining restaurant'
cuisine =  'american'
allergy_food = 'im allergic to Dairy'

# Call the function and store the result
result_df = longfamouscusinebased(location, sentence, cuisine, allergy_food)

# Print the parameters passed to the function
for index, row in result_df.iterrows():
    print("\nRestaurant Name:", row['Restaurant_Name'])
    print("Restaurant Description:", row['Restaurant_description'])
    print("Restaurant Type:", row['Restaurant_type'])
    print("Recommended Dishes:", row['recommended_dishes'])
    print("Menu with allergies:", row['menu_with_allergy'])
    print("Menu without allergies:", row['menu_without_allergy'])
    print("Allergy food product:", row['allergy_food_product'])
    print("Cost per person:", row['Cost_per_person'])
    print("Reviews:", row['reviews'])
    print("Similarity", row['similarity'])
    print("---------------------------------------------")
    pass

#Location must be required
#location and restaurant type

def longrestauranttypebased(Location, sentence):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()
    restaurants['Cleaned_Restaurant_Type'] = restaurants['Cleaned_Restaurant_Type'].str.lower()

    sentence = sentence.lower()
    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()
    sentence_tokens = word_tokenize(sentence)
    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants[restaurants['Cleaned_Location'] == Location.lower()].copy()
    similarities = []
    for index, row in reqbased.iterrows():
        temp_tokens = word_tokenize(row['Cleaned_Restaurant_Type'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]
        rvector = set(temp_tokens).intersection(set(sentence_tokens))
        similarities.append(len(rvector))
    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    recommendations = []
    for index, row in reqbased.head(10).iterrows():
        recommendation = {
            'Restaurant_Name': row['Restaurant_Name'],
            'Restaurant_description': row['Restaurant_description'],
            'Restaurant_type': row['Restaurant_type'],
            'recommended_dishes': row['recommended_dishes'],
            'menu_with_allergy': row['menu_with_allergy'],
            'menu_without_allergy': row['menu_without_allergy'],
            'allergy_food_product': row['allergy_food_product'],
            'Cost_per_person': row['Cost_per_person'],
            'reviews': row['reviews'],
            'similarity': row['similarity']
        }
        recommendations.append(recommendation)

    return recommendations

# Define parameters
location = 'New York City East Village Manhattan'
sentence = 'i want to eat from a Casual Dining restaurant'

# Call the function and store the result
result_recommendations = longrestauranttypebased(location, sentence)

# Print the recommendations
for recommendation in result_recommendations:
    print("\nRestaurant Name:", recommendation['Restaurant_Name'])
    print("Restaurant Description:", recommendation['Restaurant_description'])
    print("Restaurant Type:", recommendation['Restaurant_type'])
    print("Recommended Dishes:", recommendation['recommended_dishes'])
    print("Menu with Allergies:", recommendation['menu_with_allergy'])
    print("Menu without Allergies:", recommendation['menu_without_allergy'])
    print("Allergy Food Product:", recommendation['allergy_food_product'])
    print("Cost per Person:", recommendation['Cost_per_person'])
    print("Reviews:", recommendation['reviews'])
    print("Similarity:", recommendation['similarity'])
    print("---------------------------------------------")

####################################Location must be required
#location ,restaurant type,famouse cuisines,allergy food product and cost_per_person

def longfamouscusinebaseddddd(Location, sentence, cuisine_sentence,allergy_food_sentence,Cost_per_person_sentence):
    restaurants['Cleaned_Location'] = restaurants['Cleaned_Location'].str.lower()
    restaurants['Cleaned_Restaurant_Type'] = restaurants['Cleaned_Restaurant_Type'].str.lower()
    restaurants['Cleaned_Famous_Cuisines'] = restaurants['Cleaned_Famous_Cuisines'].str.lower()
    restaurants['allergy_food_product'] = restaurants['allergy_food_product'].str.lower()
    restaurants['Cost_per_person'] = restaurants['Cost_per_person'].str.lower()


    sentence = sentence.lower()
    cuisine_sentence = cuisine_sentence.lower()
    allergy_food_sentence = allergy_food_sentence.lower()
    Cost_per_person_sentence = Cost_per_person_sentence.lower()

    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()

    sentence_tokens = word_tokenize(sentence)
    cuisine_tokens = word_tokenize(cuisine_sentence)
    allergy_tokens = word_tokenize(allergy_food_sentence)
    Cost_tokens = word_tokenize(Cost_per_person_sentence)

    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]
    cuisine_tokens = [lemm.lemmatize(word) for word in cuisine_tokens if word.isalnum() and word not in sw]
    allergy_tokens = [lemm.lemmatize(word) for word in allergy_tokens if word.isalnum() and word not in sw]
    Cost_tokens =[lemm.lemmatize(word) for word in Cost_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants[restaurants['Cleaned_Location'] == Location.lower()].copy()
    similarities = []
    for index, row in reqbased.iterrows():
        # Tokenize the restaurant type and famous cuisines
        temp_tokens = word_tokenize(row['Cleaned_Restaurant_Type'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]
        cuisine_tokens_copy = cuisine_tokens.copy()
        allergy_tokens_copy = allergy_tokens.copy()
        Cost_tokens_copy =Cost_tokens.copy()

        # Check for common tokens in restaurant type
        rvector = set(temp_tokens).intersection(set(sentence_tokens))

        # Check for common tokens in famous cuisines
        for cuisine_token in cuisine_tokens_copy:
            if cuisine_token in row['Cleaned_Famous_Cuisines']:
                rvector.add(cuisine_token)

         # Check for common tokens in allergy food products
        for allergy_token in allergy_tokens_copy:
            if allergy_token in row['allergy_food_product']:
                rvector.add(allergy_token)

        # Check for common tokens in allergy food products
        for Cost_token in Cost_tokens_copy:
            if Cost_token in row['Cost_per_person']:
                rvector.add(Cost_token)



        similarities.append(len(rvector))


    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    return reqbased[['reviews', 'Restaurant_Name', 'Restaurant_description','Restaurant_type','recommended_dishes','menu_with_allergy','menu_without_allergy','allergy_food_product',
                     'Cost_per_person','reviews','similarity']].head(10)

    # Define the parameters
location = 'Colombo Sri Lanka'
sentence = 'i want to eat from a Fine Dining restaurant'
cuisine = 'i like Sri Lankan'
allergy = 'im alergic to pineapple'
cost = '3000'

# Call the function and store the result
result_df = longfamouscusinebaseddddd(location, sentence, cuisine, allergy, cost)

# Print the result
for index, row in result_df.iterrows():
    print("\nRestaurant Name:", row['Restaurant_Name'])
    print("Restaurant Description:", row['Restaurant_description'])
    print("Restaurant Type:", row['Restaurant_type'])
    print("Recommended Dishes:", row['recommended_dishes'])
    print("Menu with allergies:", row['menu_with_allergy'])
    print("Menu without allergies:", row['menu_without_allergy'])
    print("Allergy food product:", row['allergy_food_product'])
    print("Cost per person:", row['Cost_per_person'])
    print("Reviews:", row['reviews'])
    print("Similarity", row['similarity'])
    print("---------------------------------------------")
    pass

#part 3
#if user adked long based questions

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def longfamouscusinebasedddddd(sentence):
    restaurants['merged_info'] = restaurants[['Location', 'Restaurant_type', 'famous_Cuisines', 'allergy_food_product', 'Cost_per_person', 'recommended_dishes']].fillna('').agg(', '.join, axis=1)
    restaurants['merged_info'] = restaurants['merged_info'].str.lower()

    sentence = sentence.lower()

    sw = stopwords.words('english')
    lemm = WordNetLemmatizer()

    sentence_tokens = word_tokenize(sentence)
    sentence_tokens = [lemm.lemmatize(word) for word in sentence_tokens if word.isalnum() and word not in sw]

    reqbased = restaurants.copy()
    similarities = []
    for index, row in reqbased.iterrows():
        # Tokenize the merged information
        temp_tokens = word_tokenize(row['merged_info'])
        temp_tokens = [lemm.lemmatize(word) for word in temp_tokens if word.isalnum() and word not in sw]

        # Check for common tokens
        rvector = set(temp_tokens).intersection(set(sentence_tokens))
        similarities.append(len(rvector))

    reqbased['similarity'] = similarities
    reqbased = reqbased.sort_values(by='similarity', ascending=False)

    return reqbased[['reviews', 'Restaurant_Name', 'Restaurant_description','Restaurant_type','recommended_dishes','menu_with_allergy','menu_without_allergy','allergy_food_product',
                     'Cost_per_person','similarity']].head(10)

    # Define the parameters
sentence = 'i want to eat from a Casual Dining restaurant from sri lanka'

# Call the function and store the result
result_df = longfamouscusinebasedddddd(sentence)

# Print the parameters passed to the function
for index, row in result_df.iterrows():
    print("\nRestaurant Name:", row['Restaurant_Name'])
    print("Restaurant Description:", row['Restaurant_description'])
    print("Restaurant Type:", row['Restaurant_type'])
    print("Recommended Dishes:", row['recommended_dishes'])
    print("Menu with allergies:", row['menu_with_allergy'])
    print("Menu without allergies:", row['menu_without_allergy'])
    print("Allergy food product:", row['allergy_food_product'])
    print("Cost per person:", row['Cost_per_person'])
    print("Reviews:", row['reviews'])
    print("Similarity", row['similarity'])
    print("---------------------------------------------")
    pass

import pandas as pd
from nltk.tokenize import word_tokenize

# Read the CSV file containing bad words
badwords_df = pd.read_csv('nowords.csv')

# Assuming the column name containing bad words is 'badwords'
badwords = set(badwords_df['splitword'].str.lower())

def check_for_bad_words(sentence):
    # Tokenize the user's sentence and lowercase the words
    sentence_tokens = word_tokenize(sentence.lower())

    # Check if any of the words from the user's sentence match the bad words
    for word in sentence_tokens:
        if word in badwords:
            return True  # Bad word found

    return False  # No bad words found

# Example usage:
user_input = "I'm in New York City book."
if check_for_bad_words(user_input):
    print("I'm sorry, I'm not able to assist with that. Could you please ask me something related to restaurants?")
    pass

# Store cleaned functions and data in a dictionary
restaurant_model_data = {
    'clean_restaurant_name': clean_restaurant_name,
    'clean_description': clean_description,
    'clean_location': clean_location,
    'clean_restaurant_type': clean_restaurant_type,
    'clean_recommended_dishes': clean_recommended_dishes,
    'clean_famous_cuisines': clean_famous_cuisines,
    'clean_allergy_food_product': clean_allergy_food_product,
    'clean_menu_without_allergy': clean_menu_without_allergy,
    'clean_menu_with_allergy': clean_menu_with_allergy,
    'clean_reviews': clean_reviews,
    'longfamouscusinebased': longfamouscusinebased,
    'longfamouscusinebased': longfamouscusinebased,
    'locationbased': locationbased,
    'longfamouscusinebased': longfamouscusinebased,
    'longrestauranttypebased': longrestauranttypebased,
    'longfamouscusinebased': longfamouscusinebased,
    'check_for_bad_words': check_for_bad_words,
    'restaurants': restaurants
}



